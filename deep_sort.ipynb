{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s9554\\miniconda3\\envs\\yolo\\lib\\site-packages\\torchreid\\reid\\metrics\\rank.py:11: UserWarning: Cython evaluation (very fast so highly recommended) is unavailable, now use python evaluation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from strong_sort.strong_sort import StrongSORT\n",
    "model = tf.lite.Interpreter(r'models\\yolo11n_float32.tflite')\n",
    "model.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: osnet_x0_25\n",
      "- params: 203,568\n",
      "- flops: 82,316,000\n",
      "Successfully loaded pretrained weights from \"models\\osnet_x0_25_msmt17.pt\"\n",
      "** The following layers are discarded due to unmatched keys or layer size: ['classifier.weight', 'classifier.bias']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s9554\\miniconda3\\envs\\yolo\\lib\\site-packages\\torchreid\\reid\\utils\\torchtools.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fpath, map_location=map_location)\n"
     ]
    }
   ],
   "source": [
    "tracker = StrongSORT(\n",
    "    model_weights = r'models\\osnet_x0_25_msmt17.pt',\n",
    "    device = \"cpu\",\n",
    "    fp16 = True,\n",
    "    max_dist=0.2,  # Appearance matching threshold (lower = stricter)\n",
    "    max_iou_distance=0.7,\n",
    "    max_age=70,  # Frames to keep lost tracks\n",
    "    n_init=3,  # Frames to confirm a track\n",
    "    nn_budget=100,\n",
    "    mc_lambda=0.995,\n",
    "    ema_alpha=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepSort(frame):\n",
    "    input_details = model.get_input_details()[0]\n",
    "    output_details = model.get_output_details()[0]\n",
    "\n",
    "    frame_resized = cv.resize(frame, (input_details['shape'][2], input_details['shape'][1]))\n",
    "    frame_norm = frame_resized.astype(np.float32) / 255.0\n",
    "    model.set_tensor(input_details['index'], np.expand_dims(frame_norm, axis=0))\n",
    "    model.invoke()\n",
    "    \n",
    "    raw_output = model.get_tensor(output_details['index']).squeeze().T\n",
    "\n",
    "    class_probs = raw_output[:, 4:]\n",
    "    confidence_score = np.max(class_probs, axis=1)\n",
    "    class_id = np.argmax(class_probs, axis=1)\n",
    "    bbox = raw_output[:, 0:4]\n",
    "\n",
    "    conf_thres = 0.4\n",
    "    mask = (confidence_score >= conf_thres) & (class_id == 0)\n",
    "    filtered_box = bbox[mask]\n",
    "    filtered_score = confidence_score[mask]\n",
    "\n",
    "    if filtered_box.size == 0:\n",
    "        return\n",
    "\n",
    "    height, width = frame.shape[:2]\n",
    "    x_center, y_center, w, h = filtered_box[:, 0], filtered_box[:, 1], filtered_box[:, 2], filtered_box[:, 3]\n",
    "    x1 = (x_center - w / 2) * width\n",
    "    y1 = (y_center - h / 2) * height\n",
    "    x2 = (x_center + w / 2) * width\n",
    "    y2 = (y_center + h / 2) * height\n",
    "\n",
    "    boxes_nms = np.stack([x1, y1, x2 - x1, y2 - y1], axis=1).astype(np.float32).tolist()\n",
    "\n",
    "    nms_thres = 0.4\n",
    "    indices = cv.dnn.NMSBoxes(\n",
    "        bboxes=boxes_nms,\n",
    "        scores=filtered_score.tolist(),\n",
    "        score_threshold=conf_thres,\n",
    "        nms_threshold=nms_thres\n",
    "    )\n",
    "\n",
    "    detection = []\n",
    "    if len(indices) > 0:\n",
    "        indices = indices.flatten()\n",
    "        for i in indices:\n",
    "            x1_i, y1_i, x2_i, y2_i = int(x1[i]), int(y1[i]), int(x2[i]), int(y2[i])\n",
    "            detection.append([x1_i, y1_i, x2_i - x1_i, y2_i - y1_i])\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "    detection = np.array(detection)\n",
    "\n",
    "    tracks = tracker.update(\n",
    "        bbox_xywh = detection,\n",
    "        classes = 0,\n",
    "        confidences = filtered_score,\n",
    "        ori_img=frame_resized\n",
    "    )\n",
    "\n",
    "    return tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sort():\n",
    "    vid = cv.VideoCapture(0)\n",
    "    \n",
    "    while vid.isOpened():\n",
    "        _, frame = vid.read()\n",
    "        if not _:\n",
    "            print(\"Failed to capture frame\")\n",
    "            break\n",
    "        original_h, original_w = frame.shape[:2]\n",
    "        resized_frame = cv.resize(frame, (640, 640))\n",
    "        \n",
    "        tracker_obj = deepSort(resized_frame)\n",
    "\n",
    "        for obj in tracker.tracker.tracks:\n",
    "            x1, y1, x2, y2 = obj.to_tlbr()\n",
    "            cv.rectangle(resized_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        cv.imshow(\"Tracking\", resized_frame)\n",
    "        if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "        \n",
    "    cv.destroyAllWindows()\n",
    "    vid.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mprocess_sort\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[48], line 12\u001b[0m, in \u001b[0;36mprocess_sort\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m original_h, original_w \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     10\u001b[0m resized_frame \u001b[38;5;241m=\u001b[39m cv\u001b[38;5;241m.\u001b[39mresize(frame, (\u001b[38;5;241m640\u001b[39m, \u001b[38;5;241m640\u001b[39m))\n\u001b[1;32m---> 12\u001b[0m tracker_obj \u001b[38;5;241m=\u001b[39m \u001b[43mdeepSort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresized_frame\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m tracker\u001b[38;5;241m.\u001b[39mtracker\u001b[38;5;241m.\u001b[39mtracks:\n\u001b[0;32m     15\u001b[0m     x1, y1, x2, y2 \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mto_tlbr()\n",
      "Cell \u001b[1;32mIn[47], line 53\u001b[0m, in \u001b[0;36mdeepSort\u001b[1;34m(frame)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     51\u001b[0m detection \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(detection)\n\u001b[1;32m---> 53\u001b[0m tracks \u001b[38;5;241m=\u001b[39m \u001b[43mtracker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbbox_xywh\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdetection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfidences\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfiltered_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mori_img\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_resized\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tracks\n",
      "File \u001b[1;32md:\\VOWEL #project 1\\raspi_flask_tryOne\\strong_sort\\strong_sort.py:47\u001b[0m, in \u001b[0;36mStrongSORT.update\u001b[1;34m(self, bbox_xywh, confidences, classes, ori_img)\u001b[0m\n\u001b[0;32m     45\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_features(bbox_xywh, ori_img)\n\u001b[0;32m     46\u001b[0m bbox_tlwh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_xywh_to_tlwh(bbox_xywh)\n\u001b[1;32m---> 47\u001b[0m detections \u001b[38;5;241m=\u001b[39m [Detection(bbox_tlwh[i], conf, features[i]) \u001b[38;5;28;01mfor\u001b[39;00m i, conf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[0;32m     48\u001b[0m     confidences)]\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# run on non-maximum supression\u001b[39;00m\n\u001b[0;32m     51\u001b[0m boxes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([d\u001b[38;5;241m.\u001b[39mtlwh \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m detections])\n",
      "File \u001b[1;32md:\\VOWEL #project 1\\raspi_flask_tryOne\\strong_sort\\strong_sort.py:47\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     45\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_features(bbox_xywh, ori_img)\n\u001b[0;32m     46\u001b[0m bbox_tlwh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_xywh_to_tlwh(bbox_xywh)\n\u001b[1;32m---> 47\u001b[0m detections \u001b[38;5;241m=\u001b[39m [\u001b[43mDetection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbbox_tlwh\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i, conf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[0;32m     48\u001b[0m     confidences)]\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# run on non-maximum supression\u001b[39;00m\n\u001b[0;32m     51\u001b[0m boxes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([d\u001b[38;5;241m.\u001b[39mtlwh \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m detections])\n",
      "File \u001b[1;32md:\\VOWEL #project 1\\raspi_flask_tryOne\\strong_sort\\sort\\detection.py:32\u001b[0m, in \u001b[0;36mDetection.__init__\u001b[1;34m(self, tlwh, confidence, feature)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtlwh \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(tlwh, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfidence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(confidence)\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\s9554\\miniconda3\\envs\\yolo\\lib\\site-packages\\torch\\_tensor.py:1151\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m   1150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "process_sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform():\n",
    "    vid = cv.VideoCapture(0)\n",
    "\n",
    "    while vid.isOpened():\n",
    "        ret, frame = vid.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to capture frame.\")\n",
    "            break\n",
    "\n",
    "        img_resized = cv.resize(frame, (640, 640)).astype(np.float32) / 255.0\n",
    "        inp = model.get_input_details()[0]\n",
    "        out = model.get_output_details()[0]\n",
    "\n",
    "        model.set_tensor(inp['index'], np.expand_dims(img_resized, axis=0))\n",
    "        model.invoke()\n",
    "        res = model.get_tensor(out['index']).squeeze().T\n",
    "\n",
    "        bbox = res[:, 0:4]\n",
    "        class_prob = res[:, 4:]\n",
    "        class_id = np.argmax(class_prob, axis=1)\n",
    "        confidence_score = np.max(class_prob, axis=1)\n",
    "\n",
    "        conf_thres = 0.5\n",
    "        mask = (class_id == 0) & (confidence_score >= conf_thres)\n",
    "\n",
    "        filtered_box = bbox[mask]\n",
    "        filtered_score = confidence_score[mask]\n",
    "\n",
    "        if filtered_box.size == 0:\n",
    "            print(\"No detections found.\")\n",
    "            cv.imshow(\"vid\", frame)\n",
    "            if cv.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "            continue\n",
    "\n",
    "        x_center, y_center, w, h = filtered_box[:, 0], filtered_box[:, 1], filtered_box[:, 2], filtered_box[:, 3]\n",
    "        x1 = (x_center - w / 2) * 640\n",
    "        y1 = (y_center - h / 2) * 640\n",
    "        x2 = (x_center + w / 2) * 640\n",
    "        y2 = (y_center + h / 2) * 640\n",
    "\n",
    "        nms_thres = 0.5\n",
    "        boxes_nms = np.stack([x1, y1, x2 - x1, y2 - y1], axis=1).astype(np.float32).tolist()\n",
    "        \n",
    "        indices = cv.dnn.NMSBoxes(boxes_nms, filtered_score.tolist(), conf_thres, nms_thres)\n",
    "\n",
    "        if len(indices) > 0:  # Ensure indices is not empty\n",
    "            for i in indices.flatten():\n",
    "                x1_i, y1_i, x2_i, y2_i = int(x1[i]), int(y1[i]), int(x2[i]), int(y2[i])\n",
    "                cv.rectangle(frame, (x1_i, y1_i), (x2_i, y2_i), (0, 255, 0), 2)\n",
    "\n",
    "        cv.imshow(\"vid\", frame)\n",
    "        if cv.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    vid.release()\n",
    "    cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No detections found.\n",
      "No detections found.\n",
      "No detections found.\n",
      "No detections found.\n",
      "No detections found.\n",
      "No detections found.\n",
      "No detections found.\n",
      "No detections found.\n",
      "No detections found.\n",
      "No detections found.\n",
      "No detections found.\n",
      "0\n",
      "No detections found.\n",
      "No detections found.\n",
      "No detections found.\n",
      "3\n",
      "No detections found.\n",
      "3\n",
      "No detections found.\n",
      "No detections found.\n",
      "1\n",
      "2\n",
      "No detections found.\n",
      "No detections found.\n",
      "No detections found.\n",
      "6\n",
      "7\n",
      "5\n",
      "7\n",
      "3\n",
      "2\n",
      "7\n",
      "No detections found.\n",
      "0\n",
      "7\n",
      "4\n",
      "2\n",
      "5\n",
      "2\n",
      "2\n",
      "2\n",
      "7\n",
      "2\n",
      "5\n",
      "2\n",
      "7\n",
      "7\n",
      "2\n",
      "5\n",
      "5\n",
      "2\n",
      "5\n",
      "8\n",
      "2\n",
      "2\n",
      "7\n",
      "2\n",
      "7\n",
      "8\n",
      "3\n",
      "8\n",
      "8\n",
      "2\n",
      "7\n",
      "6\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "1\n",
      "1\n",
      "3\n",
      "5\n",
      "2\n",
      "4\n",
      "6\n",
      "5\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "perform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: osnet_x0_25\n",
      "- params: 203,568\n",
      "- flops: 82,316,000\n",
      "Successfully loaded pretrained weights from \"models\\osnet_x0_25_msmt17.pt\"\n",
      "** The following layers are discarded due to unmatched keys or layer size: ['classifier.weight', 'classifier.bias']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "update() got an unexpected keyword argument 'detections'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 114\u001b[0m\n\u001b[0;32m    111\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 114\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 94\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# Update tracker with correct parameters\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m detections\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 94\u001b[0m     tracks \u001b[38;5;241m=\u001b[39m \u001b[43mtracker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdetections\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetections\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mori_img\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     96\u001b[0m     tracks \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mTypeError\u001b[0m: update() got an unexpected keyword argument 'detections'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import onnxruntime\n",
    "from strong_sort import StrongSORT\n",
    "from strong_sort.utils.parser import get_config\n",
    "\n",
    "# Load YOLOv11 ONNX model\n",
    "onnx_model_path = r\"models\\yolo11n.onnx\"\n",
    "session = onnxruntime.InferenceSession(onnx_model_path, providers=['CPUExecutionProvider'])\n",
    "\n",
    "# Initialize StrongSORT tracker with correct fp16 setting for CPU\n",
    "tracker = StrongSORT(\n",
    "    model_weights=r'models\\osnet_x0_25_msmt17.pt',\n",
    "    device=\"cpu\",\n",
    "    fp16=False  # Disabled for CPU compatibility\n",
    ")\n",
    "\n",
    "# Detection Parameters\n",
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "NMS_THRESHOLD = 0.45\n",
    "\n",
    "def preprocess_image(image, input_size=(640, 640)):\n",
    "    \"\"\"Resize and normalize image for YOLOv11 ONNX model.\"\"\"\n",
    "    img = cv2.resize(image, input_size)\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "    img = np.transpose(img, (2, 0, 1))  # HWC to CHW\n",
    "    img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "    return img\n",
    "\n",
    "def postprocess_detections(outputs, img_shape, input_size=(640, 640)):\n",
    "    \"\"\"Parse YOLOv11 ONNX model output into bounding boxes.\"\"\"\n",
    "    boxes_xywh, scores, class_ids = [], [], []\n",
    "\n",
    "    preds = outputs[0].squeeze(0).T  # Shape: (8400, 84)\n",
    "\n",
    "    scale_x = img_shape[1] / input_size[0]  # Original width / model input width\n",
    "    scale_y = img_shape[0] / input_size[1]  # Original height / model input height\n",
    "\n",
    "    for pred in preds:\n",
    "        class_scores = pred[4:]\n",
    "        conf = np.max(class_scores)\n",
    "        class_id = np.argmax(class_scores)\n",
    "\n",
    "        if conf > CONFIDENCE_THRESHOLD:\n",
    "            cx, cy, w, h = pred[:4]\n",
    "            # Convert coordinates to original image scale\n",
    "            x = (cx - w/2) * scale_x\n",
    "            y = (cy - h/2) * scale_y\n",
    "            width = w * scale_x\n",
    "            height = h * scale_y\n",
    "\n",
    "            boxes_xywh.append([x, y, width, height])\n",
    "            scores.append(float(conf))\n",
    "            class_ids.append(int(class_id))\n",
    "\n",
    "    # Apply NMS with correct box format (x, y, w, h)\n",
    "    indices = cv2.dnn.NMSBoxes(boxes_xywh, scores, CONFIDENCE_THRESHOLD, NMS_THRESHOLD)\n",
    "    final_boxes, final_scores, final_class_ids = [], [], []\n",
    "\n",
    "    if len(indices) > 0:\n",
    "        for i in indices.flatten():\n",
    "            x, y, w, h = boxes_xywh[i]\n",
    "            final_boxes.append([int(x), int(y), int(x + w), int(y + h)])\n",
    "            final_scores.append(scores[i])\n",
    "            final_class_ids.append(class_ids[i])\n",
    "\n",
    "    return final_boxes, final_scores, final_class_ids\n",
    "\n",
    "def main():\n",
    "    cap = cv2.VideoCapture(0)  # Webcam input\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Preprocess and run inference\n",
    "        input_tensor = preprocess_image(frame)\n",
    "        outputs = session.run(None, {session.get_inputs()[0].name: input_tensor})\n",
    "\n",
    "        # Postprocess detections\n",
    "        boxes, scores, class_ids = postprocess_detections(outputs, frame.shape)\n",
    "\n",
    "        # Prepare detections for tracking\n",
    "        detections = []\n",
    "        for box, score, class_id in zip(boxes, scores, class_ids):\n",
    "            x1, y1, x2, y2 = box\n",
    "            detections.append([x1, y1, x2-x1, y2-y1, score, class_id])\n",
    "\n",
    "        detections = np.array(detections, dtype=np.float32) if detections else np.empty((0, 6), dtype=np.float32)\n",
    "\n",
    "        # Update tracker with correct parameters\n",
    "        if detections.size > 0:\n",
    "            tracks = tracker.update(detections=detections, ori_img=frame)\n",
    "        else:\n",
    "            tracks = []\n",
    "\n",
    "        # Draw tracked objects\n",
    "        for track in tracks:\n",
    "            track_id = track.track_id\n",
    "            x, y, w, h = track.to_tlwh().astype(int)\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"ID {track_id}\", (x, y - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "        cv2.imshow(\"Tracking\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
